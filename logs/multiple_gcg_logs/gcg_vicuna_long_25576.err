/tmp/slurmd/job25576/slurm_script: line 11: nvidea-smi: command not found
2025-08-13 14:38:39,591 - INFO - ======================================================================
2025-08-13 14:38:39,591 - INFO -                       Ataque adversarial a LLMs                       
2025-08-13 14:38:39,591 - INFO - ======================================================================
2025-08-13 14:38:39,592 - INFO - ======================================================================
2025-08-13 14:38:39,592 - INFO -                               RESULTADOS                              
2025-08-13 14:38:39,592 - INFO - ======================================================================
2025-08-13 14:38:39,592 - INFO - Importando bibliotecas. . .
2025-08-13 14:39:24,525 - INFO - Bibliotecas importadas
2025-08-13 14:39:44,010 - INFO - Usando cuda
2025-08-13 14:39:44,011 - INFO - Importando dados. . .
2025-08-13 14:39:44,102 - INFO - Dados importados
2025-08-13 14:39:44,102 - INFO - ======================================================================
2025-08-13 14:39:44,102 - INFO -                 Modelo: meta-llama/Llama-2-7b-chat-hf                 
2025-08-13 14:39:44,102 - INFO - ======================================================================
2025-08-13 14:39:44,102 - INFO - ======================================================================
2025-08-13 14:39:44,103 - INFO -                 Modelo: meta-llama/Llama-2-7b-chat-hf                 
2025-08-13 14:39:44,103 - INFO - ======================================================================
2025-08-13 14:39:44,103 - INFO - Importando modelo. . .
/home/CIN/mefc/deteccao/llm-attacks/detec/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 1/2 [29:45<29:45, 1785.91s/it]Downloading shards: 100%|██████████| 2/2 [31:54<00:00, 810.97s/it] Downloading shards: 100%|██████████| 2/2 [31:54<00:00, 957.21s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]
2025-08-13 15:37:13,448 - ERROR - Erro ao importar bibliotecas: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 9.50 GiB of which 2.00 MiB is free. Process 1159648 has 384.00 MiB memory in use. Process 1159652 has 384.00 MiB memory in use. Process 1159653 has 384.00 MiB memory in use. Process 1159651 has 384.00 MiB memory in use. Process 1159657 has 384.00 MiB memory in use. Process 1159655 has 384.00 MiB memory in use. Process 1159650 has 384.00 MiB memory in use. Process 1159654 has 384.00 MiB memory in use. Process 1159649 has 384.00 MiB memory in use. Process 1159656 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 9.48 GiB memory in use. Process 1161179 has 1.05 GiB memory in use. Of the allocated memory 8.83 GiB is allocated by PyTorch, and 1.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
